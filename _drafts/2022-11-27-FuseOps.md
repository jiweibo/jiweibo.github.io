---
layout: post
title:  "FuseOps for tvm"
date:   2022-11-27 12:00:00 +0800
description: tvm
categories: tvm
tags: [tvm]
location: Beijing,China
---

# 概述

在深度学习领域，想必大家对算子融合的概念都不陌生，算子融合通常是指将多个算子融合成单个kernel，从而避免了中间结果的访存和kernel launch的开销。本章主要介绍TVM中的算子融合，也称之为自动fusion。本文以 tvm v0.10.0 代码为例进行解读。


# 算子类型

在TVM中对算子进行了分类，代码在`op_attr_types.h`中：

```c++
/*! \brief operator pattern used in graph fusion */
enum OpPatternKind {
  // Elementwise operation
  kElemWise = 0,
  // Broadcasting operator, can always map output axis to the input in order.
  // for example :code:`out[i, ax1, j, ax2] = input[i, j]`.
  // Note that the axis need to be in order so transpose is not a bcast operator.
  kBroadcast = 1,
  // Injective operator, can always injectively map output axis to a single input axis.
  // All injective operator can still be safely fused to injective and reduction.
  kInjective = 2,
  // Communicative reduction operator.
  kCommReduce = 3,
  // Complex operation, can still fuse elemwise operations into its output.
  // but cannot chain another complex op
  kOutEWiseFusable = 4,
  // The pattern for tuple nodes. Can fuse into subsequent injective ops,
  // but treated specially
  kTuple = 7,
  // Opaque operation, cannot fuse anything.
  kOpaque = 8
};
```

目前定义了7种算子类型，

- `kElemWise`，逐元素操作的算子；
- `kBroadcast`，带有广播操作的算子；
- `kInjective`，输入输出之间具有映射关系的算子；
- `kCommReduce`，reduce计算的算子；
- `kOutEWiseFusable`，输出可与`kElemWise`进行fuse的算子，如`conv`；
- `kTuple`，操作元祖的算子，如`TupleNode`，`TupleGetItemNode`等；
- `kOpaque`，无法进行fuse的算子；

TVM 有一套算子融合的算法，能够自动融合满足特定规则的一系列算子，这个规则依赖于算子类型，后文详细介绍。

# 示例程序

我们以示例程序进行分析，跟进到代码中阅读：

```python
import tvm
from tvm import te
import tvm.relay as relay
import numpy as np


def get_relay_ir():
  shape = (1, 3, 14, 14)
  c_data = np.ones(shape).astype('float32')
  c = relay.const(c_data)

  weight = relay.var('weight', shape=(3, 3, 3, 3))
  x = relay.var('x', relay.TensorType((1, 3, 16, 16), 'float32'))
  conv = relay.nn.conv2d(x, weight)
  y = relay.add(conv, c)
  act = relay.nn.relu(y)

  mul = relay.multiply(conv, relay.const(0.5, 'float32'))
  z = act + mul
  return relay.Function([x, weight], z)


f = get_relay_ir()
mod = tvm.IRModule.from_expr(f)
print('src module:')
print(mod)

fold_const = relay.transform.FoldConstant()
mod = fold_const(mod)
print('fold_const:')
print(mod)

mod = relay.transform.FuseOps(fuse_opt_level=4)(mod)
print('fuse_ops:')
print(mod)
```

运行程序，常量折叠后的ir如下：
```
def @main(%x: Tensor[(1, 3, 16, 16), float32] /* ty=Tensor[(1, 3, 16, 16), float32] */, %weight: Tensor[(3, 3, 3, 3), float32] /* ty=Tensor[(3, 3, 3, 3), float32] */) -> Tensor[(1, 3, 14, 14), float32] {
  %0 = nn.conv2d(%x, %weight, padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 3, 14, 14), float32] */;
  %1 = add(%0, meta[relay.Constant][0] /* ty=Tensor[(1, 3, 14, 14), float32] */) /* ty=Tensor[(1, 3, 14, 14), float32] */;
  %2 = nn.relu(%1) /* ty=Tensor[(1, 3, 14, 14), float32] */;
  %3 = multiply(%0, 0.5f /* ty=float32 */) /* ty=Tensor[(1, 3, 14, 14), float32] */;
  add(%2, %3) /* ty=Tensor[(1, 3, 14, 14), float32] */
}
```

算子融合后的ir如下：
```
def @main(%x: Tensor[(1, 3, 16, 16), float32] /* ty=Tensor[(1, 3, 16, 16), float32] */, %weight: Tensor[(3, 3, 3, 3), float32] /* ty=Tensor[(3, 3, 3, 3), float32] */) -> Tensor[(1, 3, 14, 14), float32] {
  %4 = fn (%p0: Tensor[(1, 3, 16, 16), float32] /* ty=Tensor[(1, 3, 16, 16), float32] */, %p1: Tensor[(3, 3, 3, 3), float32] /* ty=Tensor[(3, 3, 3, 3), float32] */, %p2: Tensor[(1, 3, 14, 14), float32] /* ty=Tensor[(1, 3, 14, 14), float32] */, Primitive=1) -> Tensor[(1, 3, 14, 14), float32] {
    %0 = nn.conv2d(%p0, %p1, padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 3, 14, 14), float32] */;
    %1 = add(%0, %p2) /* ty=Tensor[(1, 3, 14, 14), float32] */;
    %2 = nn.relu(%1) /* ty=Tensor[(1, 3, 14, 14), float32] */;
    %3 = multiply(%0, 0.5f /* ty=float32 */) /* ty=Tensor[(1, 3, 14, 14), float32] */;
    add(%2, %3) /* ty=Tensor[(1, 3, 14, 14), float32] */
  } /* ty=fn (Tensor[(1, 3, 16, 16), float32], Tensor[(3, 3, 3, 3), float32], Tensor[(1, 3, 14, 14), float32]) -> Tensor[(1, 3, 14, 14), float32] */;
  %4(%x, %weight, meta[relay.Constant][0] /* ty=Tensor[(1, 3, 14, 14), float32] */) /* ty=Tensor[(1, 3, 14, 14), float32] */
}
```

对比上述两ir可知，算子融合pass后，conv，add，relu和multiply算子被融合成一个算子，在TVM中为`CallNode`。

# 算子融合调用入口

算子融合pass的python入口在[transform.py](https://github.com/apache/tvm/blob/v0.10.0/python/tvm/relay/transform/transform.py#L305)：
```python
def FuseOps(fuse_opt_level=-1):
    """Fuse operators in an expr to a larger operator according to some rules.

    Parameters
    ----------
    fuse_opt_level : int
        The level of fuse optimization. -1 indicates that the level will be
        inferred from pass context.

    Returns
    -------
    ret : tvm.transform.Pass
        The registered pass for operator fusion.
    """
    return _ffi_api.FuseOps(fuse_opt_level)
```

TVM通过 packed_func ffi 机制实现了 python 和 c++ 之间的相互调用，其 c++ 后端代码在[fuse_ops.cc](https://github.com/apache/tvm/blob/v0.10.0/src/relay/transforms/fuse_ops.cc#L1066)。

```c++
Pass FuseOps(int fuse_opt_level) {
  runtime::TypedPackedFunc<Function(Function, IRModule, PassContext)> pass_func =
      [=](Function f, IRModule m, PassContext pc) {
        bool link_params = false;
        Executor executor =
            m->GetAttr<Executor>(tvm::attr::kExecutor).value_or(NullValue<Executor>());
        link_params = executor.defined()
                          ? executor->attrs.GetAttr<Bool>("link-params").value_or(Bool(link_params))
                          : link_params;
        link_params = pc->GetConfig("relay.FuseOps.link_params", Bool(link_params)).value();
        int opt_level = fuse_opt_level == -1 ? pc->opt_level : fuse_opt_level;
        auto max_fuse_depth = pc->GetConfig("relay.FuseOps.max_depth", Integer(kMaxFusedOps));
        return Downcast<Function>(
            FuseOps(f, opt_level, max_fuse_depth.value().IntValue(), link_params, m));
      };
  return CreateFunctionPass(pass_func, 0, "FuseOps", {"InferType"});
}
```
此处目前只关注`opt_level`优化级别选项即可，可通过`PassContext`进行设置，其余参数暂未用到，使用其默认值即可。

# 自动fuse实现思想

代码注释中有提到，其实现思路如下：

- relay树ir 构建DAG，用于方便节点支配分析；
- 构造后支配树，能够快速求取任意节点的后支配点；
- 根据当前节点的后支配点信息，在两节点路径之间运行融合算法；

主体代码如下：
```c++
  Expr Transform(const Expr& body) {
    return Transform(body, fuse_opt_level_, max_fuse_depth_, link_params_);
  }

  // Run the transform
  Expr Transform(const Expr& body, int fuse_opt_level, size_t max_fuse_depth, bool link_params) {
    // setup the group map.
    auto graph = IndexedForwardGraph::Create(&arena_, body);
    auto groups = GraphPartitioner(&arena_, fuse_opt_level, max_fuse_depth).Partition(graph);
    for (size_t nid = 0; nid < graph.post_dfs_order.size(); ++nid) {
      ICHECK(graph.post_dfs_order[nid]->ref != nullptr);
      gmap_[graph.post_dfs_order[nid]->ref] = groups[nid];
    }
    // The following line can be used for debug.
    // this->DebugDumpGroup(body);
    return this->Mutate(body);
  }
```

## 构建DAG

构建DAG主要由以下代码完成：
```c++
auto graph = IndexedForwardGraph::Create(&arena_, body);
```
其中，`arena_`为内存管理模块，可以先不关心，`body`为relay的树ir，此处是一个`FunctionNode`；

`IndexedForwardGraph`类似于一般深度学习框架中Graph, Node结构的IR表示：
```c++
/*!
 * \brief Indexed data flow graph in forward direction.
 *  This is a temporary data structure used for operator fusion analysis.
 *
 *  This data structure only captures the dataflow fragment and
 *  could ignore blocks like let by simply ordering each dataflow block
 *  and mark the output node as extern_ref;
 */
class IndexedForwardGraph {
 public:
  struct Node;
  /*!
   * The forward edge in the dataflow graph.
   */
  struct Edge {
    /*! \brief The corresponding node */
    Node* node{nullptr};
    /*! \brief The respective pattern of this op */
    OpPatternKind pattern{kOpaque};
  };
  /*! \brief A node in the graph. */
  struct Node {
    /*! \brief weak reference to the corresponding edge. */
    const tvm::Object* ref{nullptr};
    /*! \brief The index of the node in topological order. */
    size_t index{0};
    /*! \brief Whether this node is referenced by external source */
    bool extern_ref{false};
    /*! \brief The general pattern in the node */
    OpPatternKind pattern{kOpaque};
    /*! \brief The outputs of the node. */
    LinkedList<Edge> outputs;
  };
  /*! \brief The node map that maps node to graph */
  std::unordered_map<const tvm::Object*, Node*> node_map;
  /*! \brief All the nodes in post DFS order */
  std::vector<Node*> post_dfs_order;
  ...
};
```

该`Node`节点存储了引用对象`ref`，拓扑序`index`，算子类型`pattern`，是否被引用`extern_ref`以及与节点输出的边`outputs`这些信息；`IndexedForwardGraph`还存储了对象和节点的映射关系`node_map`，所有节点的post-dfs遍历顺序`post_dfs_order`。
简单理解就是该类做了一个数据结构的转换，将relay树ir转为Graph node的ir，主要通过`IndexedForwardGraph::Creator`实现；

`Creator`继承自`ExprVisitor`（此处不介绍），主要对`FunctionNode`，`CallNode`，`ConstantNode`，`VarNode`等节点的遍历进行了重写；用户传入的`body`是一个`FunctionNode`，因此首先进入`FunctionNode`的处理逻辑：
```c++
  // Post order tree
  void VisitExpr_(const FunctionNode* op) final {
    // Skip the function that should be handled by external codegen.
    if (op->GetAttr<String>(attr::kCompiler).defined()) return;

    for (auto param : op->params) {
      this->Update(param, nullptr, kOpaque);
    }
    this->Update(op->body, nullptr, kOpaque);
    ExprVisitor::VisitExpr_(op);
  }
```
其逻辑首先对参数和函数体进行了`Update`，之后进入父类的`VisitExpr_`方法中进行递归遍历。`Update`过程即为`Graph`中创建或更新`Node`的操作，如果有`parent`参数，则创建`Edge`，其代码如下：
```c++
  // Update the message stored at the node.
  void Update(const Expr& node, IndexedForwardGraph::Node* parent, OpPatternKind pattern) {
    const tvm::Object* key = node.get();
    IndexedForwardGraph::Node* current;
    auto it = graph_.node_map.find(key);
    if (it != graph_.node_map.end()) {
      current = it->second;
    } else {
      current = arena_->make<IndexedForwardGraph::Node>();
      graph_.node_map[key] = current;
    }
    if (parent != nullptr) {
      auto* link = arena_->make<LinkNode<IndexedForwardGraph::Edge>>();
      link->value.node = parent;
      link->value.pattern = pattern;
      current->outputs.Push(link);
    } else {
      current->extern_ref = true;
    }
  }
```
父类的`VisieExpr_`方法首先访问`FunctionNode`的参数：`%x`和`%weight`，更新节点信息，可知，`%x`的拓扑序是0，`%weight`的拓扑序是1，且更新了`graph`的post-dfs顺序；
```c++
  void VisitExpr_(const VarNode* op) final { this->AddNode(op); }

  void AddNode(const tvm::Object* key) {
    auto it = graph_.node_map.find(key);
    ICHECK(it != graph_.node_map.end()) << "Cannot find node " << GetRef<ObjectRef>(key);
    IndexedForwardGraph::Node* node = it->second;
    ICHECK(node->ref == nullptr);
    node->ref = key;
    node->index = graph_.post_dfs_order.size();
    graph_.post_dfs_order.push_back(node);
  }
```

接下来访问`FunctionNode`的函数体`body`，它是一个`CallNode`节点，如下所示：
```
add(%2, %3)
```
因此会进入到以下代码段：
```c++
void VisitExpr_(const CallNode* call) final {
  ......
  OpPatternKind op_pattern = kOpaque;
  if (const OpNode* opnode = call->op.as<OpNode>()) {
    auto op = GetRef<Op>(opnode);
    if (IsDynamic(call->checked_type()) && IsDataDependent(call)) {
      // output of a shape func can't be fed to a data-dependent shape func
      op_pattern = kOpaque;
    } else {
      op_pattern = static_cast<OpPatternKind>(fpattern[op]);
    }
  } else {
    this->Update(call->op, node, kOpaque);
  }

  node->pattern = op_pattern;
  this->Update(call->op, nullptr, kOpaque);
  const auto* rtype = call->checked_type().as<TensorTypeNode>();
  // pass the analysis back to all the children it references.
  for (size_t i = 0; i < call->args.size(); ++i) {
    const auto* arg_type = call->args[i]->checked_type().as<TensorTypeNode>();
    // specifically check if result type is the same as arguments type
    OpPatternKind edge_pattern = op_pattern;
    if (edge_pattern == kBroadcast && arg_type != nullptr && rtype != nullptr &&
        attr_equal_(rtype->shape, arg_type->shape)) {
      edge_pattern = kElemWise;
    }
    this->Update(call->args[i], node, edge_pattern);
  }
  ExprVisitor::VisitExpr_(call);
  this->AddNode(call);
}
```
访问到`CallNode`后，其是一个`Add Op`节点，从全局注册中找到其算子类型为`kBroadcast`，并通过`Update`将`Add Op`节点添加到`graph`中；接下来处理输入args，此处有一个判断如果输入arg的Shape与返回值Shape一致，则算子类型可由`kBroadcast`转为`kElemWise`，之后更新arg节点，建立arg到`CallNode (Call(Add, ...))`的边

TODO画图。

## 构建后支配树

## 融合规则

# 更新 IR


# References

本文主要参考了以下资料，在此表示感谢！

1. https://arxiv.org/pdf/1802.04799.pdf